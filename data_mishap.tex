\documentclass[12pt,t]{beamer}
\usepackage{graphicx}
\setbeameroption{hide notes}
\setbeamertemplate{note page}[plain]
\usepackage{listings}
\usepackage{eepic}

\input{header.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% end of header
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% title info
\title{A data mishap}
\subtitle{Estimating allele frequencies in sibships}
\author{\href{https://kbroman.org}{Karl Broman}}
\institute{Biostatistics \& Medical Informatics, UW{\textendash}Madison}
\date{\href{https://kbroman.org}{\tt \scriptsize \color{foreground} kbroman.org}
\\[-4pt]
\href{https://github.com/kbroman}{\tt \scriptsize \color{foreground} github.com/kbroman}
\\[-4pt]
\href{https://twitter.com/kwbroman}{\tt \scriptsize \color{foreground} @kwbroman}
\\[-4pt]
{\scriptsize Course web: \href{https://kbroman.org/AdvData}{\tt kbroman.org/AdvData}}
}


\begin{document}

% title slide
{
\setbeamertemplate{footline}{} % no page number here
\frame{
  \titlepage

\note{
}

} }


\begin{frame}{GWAS for ``{\hilit morning person}''}

  \bigskip
  \bigskip

\figh{Figs/GWAS_morning_person.jpeg}{0.6}

  \bigskip
  \bigskip

\vfill
\hfill \footnotesize {\lolit Hu et al (2016)
  \href{https://doi.org/10.1038/ncomms10448}{doi:10.1038/ncomms10448}}

\note{
}

\end{frame}




\begin{frame}{BRCA pedigree}

  \bigskip

\figh{Figs/brca_family.png}{0.7}

\vfill
\hfill \footnotesize {\lolit Hall et al (1990)
  \href{https://doi.org/10.1126/science.2270482}{doi:10.1126/science.2270482}}

\note{
}

\end{frame}





\begin{frame}[c]{Affected sib pairs}
\only<1|handout 0>{\figw{Figs/sibpairs.pdf}{1.0}}
\only<2>{\figw{Figs/sibpairs_wdata.pdf}{1.0}}

\note{
  In the late 1990s, one of ways we tried to identify disease genes
  was with affected sibling pair studies. You gather a bunch of pairs
  of siblings who were both affected with a disease, and then get
  genotype data for them, and look for genomic regions where the
  affected sibpairs had more similar genotypes than you would expect
  by chance.
}

\end{frame}


\begin{frame}[c]{IBS vs IBD}


\large

IBS = identical by {\hilit state}

{\color{background} IBS}    = same allele number \\[18pt]

  IBD = identical by {\hilit descent}

{\color{background} IBD} = copies of the same ancestral allele \\[36pt]

non-inbred sibs are IBD = 0, 1, 2

{\color{background} non-inbred sibs} with probability = 1/4, 1/2, 1/4



\note{
   In measuring genetic similarity at a locus, it's valuable to
   distinguish between alleles being ``identical by state'' (meaning
   they just look the same) and ``identical by descent'' (meaning that
   they are copies of an ancestral allele). Non-inbred siblings will
   have IBD status 0, 1, or 2, with probability 1/4, 1/2, 1/4,
   respectively.
}

\end{frame}

\begin{frame}{Prostate cancer genome scan}

\bigskip

\figh{Figs/gh_results_bad.pdf}{0.75}

\note{
  In an affected sib-pair study, we'll scan across the genome; at each
  point will seek to estimate the proportion of alleles shared IBD
  between affected siblings and compare that to what is expected for
  siblings.

  The first such study I was involved in was of prostate cancer, and
  consisted of maybe 150 affected sibling pairs. This plot (of
  $-\log_{10}$ p-values) is an approximation of my initial results.
  We're looking for values around 3, so these were super exciting to
  me. I distinctly remember faxing these results to my collaborators,
  thinking ``I am so awesome. I will conquer all diseases.''
}

\end{frame}


\begin{frame}{}

  \vspace*{-0.5in} \hspace*{-1.0in}

\figh{Figs/moustache.jpeg}{1.12}

\vspace*{-0.5in}
\hfill
{\large \color{white} so happy} \hspace*{0.2in}

\end{frame}



\begin{frame}[c]{Lesson}

\centering
\Large
If it seems too good to be true, \\[12pt]
it probably is.


\note{
  But as soon as I sent that fax, I was like, ``Huh. Those results
  seem too good to be true.''

  It turns out that I'd messed up the allele frequencies and so the
  results were all messed up.
}

\end{frame}



\begin{frame}[c]{Prostate cancer pairs}
\only<1|handout 0>{\figw{Figs/sibpairs_nopar.pdf}{1.0}}
\only<2>{\figw{Figs/sibpairs_nopar_wdata.pdf}{1.0}}

\note{
   In this prostate cancer study, the affected sibpairs are all old,
   and there's essentially no data on the parents. In this case,
   determining the number of alleles shared IBD turns out to be
   particularly sensitive to the allele frequencies.

   For example, if they're both 3/3 and 3 is relatively rare, that
   it's very likely that they're IBD=2. But if 3 is quite common, then
   they could reasonably be IBD=1 or 0.

   If they both share a 6 allele, is that IBD=1, or IBD=0? If 6 is
   rare, you'd lean towards IBD=1, but if 6 is common, it could be
   either.
}

\end{frame}



\begin{frame}[c]{Prostate cancer genome scan -- corrected}
\figw{Figs/gh_results_good.pdf}{1.0}

\note{
   The unusually strong results I got were entirely due to a mistake
   in the code that estimated the allele frequencies. If I use more
   reasonable estimates, this is what I get. There's maybe evidence
   for a disease locus on chr 16 and possibly also 15, but the
   evidence isn't very strong.

   And this is sort of what we'd expect given the size of this study.
   We're hoping to find some evidence of a disease gene, but we're not
   going to see the whole genome lighting up.
}

\end{frame}


\begin{frame}{Estimating allele frequencies}

  \bigskip

  Usually, you would use the {\hilit founders} in the pedigrees. \\
  \qquad (assumed unrelated)

  \bigskip \bigskip

  What if you only have {\vhilit sibships}?


\note{
    And just how should we estimate the allele frequencies? The normal
    way is to use the founders founders in the pedigrees (here, the
    parents), who we would generally assume to be unrelated. But in
    this case the parents are entirely absent. We just have data on
    the siblings themselves.
}

\end{frame}



\begin{frame}{Estimating allele frequencies with sibpairs}

  \bigskip

  {\hilit Method 1}: Use a random sibling from each

  \onslide<2->{

    $$ \text{var}(\hat{p}^{(1)}) = \frac{p(1-p)}{2n}$$

  }


  \bigskip \bigskip \bigskip

  {\hilit Method 2}: Use everyone, ignoring relationships


  \onslide<2->{

    $$ \text{var}(\hat{p}^{(2)}) = \only<2|handout 0>{\, \text{\bf ?}}
    \only<3->{{\vhilit (3/4)} \, \left(\frac{p(1-p)}{2n}\right)}
    $$
  }

  \only<4->{\qquad relative efficiency = 4/3 = 1.33

    \qquad \qquad {\lolit (best possible = 1.5)}}


\note{
   Well, one way is to just use a single sibling from each pair. Then
   you have a set of unrelated individuals. Alternatively, you could
   use the data on all of the siblings. But then you are potentially
   overcounting some alleles.

   Which one is the better estimate? Is it better to omit some data so
   that what's left is a set of pure, independent counts? Or is it
   better to use all of the data and ignoring the over-counting of
   alleles.

   Both methods give unbiased estimates. And the standard error for
   the first estimate is that of a binomial proportion with sample
   size 2$n$, where $n$ is the number of sibling pairs, since each
   individual gives you data on 2 alleles.

   I'll show in a moment that the second estimate has variance that is
   3/4 that of the first estimate, so a relative efficiency of 4/3.
   The best possible estimate you can get with sibling pair data has a
   relative efficiency of 3/2, since two siblings are on average
   giving you data on three distinct alleles.
}

\end{frame}



\begin{frame}[c]{My favorite equations}
  \begin{eqnarray*}
    \text{E}(X) & = & \text{E}[ \text{E}(X|Z) ] \\[18pt]
    \text{var}(X) & = & \text{E}[ \text{var}(X|Z) ] + \text{var}[ \text{E}(X|Z) ] \\[18pt]
    \text{cov}(X,Y) & = & \text{E}[ \text{cov}(X,Y|Z) ] + \text{cov}[ \text{E}(X|Z), \text{E}(Y|Z) ]
  \end{eqnarray*}

  \bigskip\bigskip

  \hfill {\lolit Everything is a mixture}


\note{
   How do we determine the variance of that second estimate? Well it
   turns out that we'll be using some of my favorite equations,
   particularly that the variance of a random variable can be
   expressed as the mean conditional variance plus the variance of the
   conditional mean.
}

\end{frame}


\begin{frame}[c]{Another fave}
  \begin{eqnarray*}
    \text{cov}(X, aY+bZ) & = & a \, \text{cov}(X,Y) + b \, \text{cov}(X,Z) \\[18pt]
    \text{{\lolit thus} \, var}(X+Y) & = & \text{cov}(X+Y,X+Y) \\
    & = & \text{cov}(X+Y, X) + \text{cov}(X+Y, Y) \\
    & = & \text{cov}(X, X) + \text{cov}(X, Y) + \text{cov}(Y,X) + \text{cov}(Y,Y) \\
    & = & \text{var}(X) + \text{var}(Y) + 2 \, \text{cov}(X,Y)
    \end{eqnarray*}

\note{
   Here's another of my favorite equations, which is what you need in
   order to remember that the variance of the sum is the sum of the
   variances plus twice the covariance.
}

\end{frame}


\begin{frame}[c]{}
  \figh{Figs/mixture_univariate.pdf}{1.0}

\note{
   Regarding those initial equations, one way to think about them is
   of data that are mixtures. If there are two types of individuals,
   each following a different normal distribution, then the overall
   average is the average of the two individual averages, and the
   overall variance is the average of the variances plus the variance
   of the averages.
}

\end{frame}

\begin{frame}[c]{}
  \figh{Figs/mixture_bivariate.pdf}{1.0}

\note{
  For the covariance, think of a pair of correlated variables where
  again the data are a mixture of a set of underlying bivariate
  distributions. The overall covariance is the average of the
  individual covariances plus the covariance of the averages.
}

\end{frame}



\begin{frame}{Back to that SE}


  Let X$_i$ = number of 1 alleles in sib $i$.

  \bigskip

  We want $\text{var}(X_1 + X_2)$

  \bigskip

  \begin{eqnarray*}
  \text{And so really } {\hilit \text{cov}(X_1, X_2)} & = & \text{E}[\text{cov}(X_1, X_2 | \text{IBD})] + \text{cov}[E(X_1|\text{IBD}), E(X_2|\text{IBD})] \\[12pt]
   & = & {\hilit \text{E}[\text{cov}(X_1, X_2|\text{IBD})]} \\[12pt]
   & = & \sum_{k=0}^{2} \text{cov}(X_1, X_2|\text{IBD}=k) \, \text{Pr}(\text{IBD}=k)
   \end{eqnarray*}


\note{
   Now back to that SE, we can focus on a particular allele, say
   allele 1. And then we count the number of 1 alleles in each
   sibling. We want the variance of the sum, which means we're going
   to want to covariance of the counts for the two siblings. And it
   turns out to be easiest to look at this in terms of the conditional
   covariance, given the number of alleles that the two siblings share
   IBD.

   $\text{E}(X_i|\text{IBD})$ doesn't depend on the IBD status, so
   this ends up just being a number, and so the covariance of the two
   conditional means is 0, so that term drops out.
}

\end{frame}



\begin{frame}{Also}

\bigskip

  \begin{eqnarray*}
    \text{cov}(X,Y) & = & \text{E}(XY) - \text{E}(X) \text{E}(Y) \\[24pt]
    \text{cov}(X,Y|Z) & = & \text{E}(XY|Z) - \text{E}(X|Z) \text{E}(Y|Z)
   \end{eqnarray*}


\note{
   Similar to the common expansion of the variance, we can write the
   covariance as the expected product minus the product of the two
   expected values, and this works whether you're conditioning on some
   other variable or not.
}

\end{frame}



\begin{frame}[c]{}

  \figh{Figs/broman2001_table4.png}{0.9}

\vspace{3mm}

\hfill \footnotesize {\lolit Broman (2001)
  \href{https://doi.org/10.1002/gepi.2}{doi:10.1002/gepi.2}}


\note{
   The basis of the calculation is figuring out the conditional
   distribution of $(X_1, X_2)$ given IBD status, shown here.
   Remember $X_i$ is the number of 1 alleles in the $i$th sib.

   Consider the case IBD=2. Here the two sibs have the same genotypes,
   and so $X_1=X_2$. The frequencies are according to the
   Hardy-Weinberg proportions: draw 2 alleles with replacement from a
   vat with $p$ 1's,

   The case IBD=0 is similarly easy. Here the two sibs have
   independent genotypes, and so the probabilities are according to
   the case that you draw 4 alleles with replacement from a vat with
   $p$ 1's.

   IBD=1 is like drawing three alleles with replacement from a vat of
   $p$ 1's. In the $X_1=1,X_2=1$ case you then need to sum over the
   two possibilities: that the allele in common between the sibs is a
   1 or that it's not.
}

\end{frame}


\begin{frame}{Lessons}

  \bbi
  \itemsep36pt
\item Omitting data is usually bad
\item Crudely ignoring correlations can be good
  \bi
\item[] You might even be able to figure out the SE
  \ei
  \ei


\note{
   The lesson that you learn here are that omitting data are ususally
   (making for a worse estimate), and that just ignoring correlations
   in the data may still give you an okay estimate. And in many cases,
   you might be able to figure out the SE of that estimate that
   ignores the correlations.

   From what I've seen, this is true quite generally. If you find
   yourself thinking, ``Should I omit some data, so what's left is all
   independent?'' you should hesitate and think back to this
   situation.

   I should also emphasize here that the analytic calculations I've
   shown are not strictly necessary. We could have just as well
   proceeded by computer simulation. We could simulate siblings'
   genotypes and then drive the two estimates and repeat many times,
   and we'd immediately get to the answers about unbiasedness and the
   4/3 relative efficiency.
}

  \end{frame}



\begin{frame}{Method 3}

{\hilit Account for relationships in the estimate}

\bigskip \bigskip

Missing data = IBD status for a sib pair at a marker

\bigskip \bigskip

Use {\vhilit EM algorithm}:

\bigskip

\hspace*{15mm} \begin{minipage}{8in}
\bi
\item[\hilit E step:] estimate IBD status given allele frequencies

\item[\hilit M step:] estimate allele frequencies given IBD status
\ei
\end{minipage}


\note{
  Now we {\hilit can} actually estimate the allele frequencies
  accounting for the relationships between the siblings. The key idea
  is that if we need which alleles were IBD, we could prevent the
  overcounting we get when ignoring the relationships.

  This naturally leads to yet another instance of an EM algorithm.
  In the E step, we estimated IBD status given current estimates of
  allele frequencies. In the M step, we re-estimate the allele
  frequencies, given the IBD status.
}

\end{frame}




\begin{frame}{Method 4}

{\hilit Make use of the multiple markers on a chromosome}

\bbi
\item Markers along chromosome give improved info
  about IBD status
\item Again, an EM algorithm:
  \bi
\item Estimate IBD along chromosome given allele frequencies
\item Re-estimate allele frequencies using IBD information
  \ei
\ei

\note{
  But if that method were to work, we could do even better by taking
  account of surrounding markers to get improved estimates of IBD
  status which should then give us improved estimates of allele
  frequencies.

  So again we have an EM algorithm, but at the E step we use all of
  the markers on a chromosome to infer IBD status; the M step will
  remain the same as before.
}

\end{frame}



\begin{frame}{Siblings' chromosomes}
\figw{Figs/sib_chr.pdf}{1.0}

\note{
  To illustrate the use of surrounding markers, here are the
  chromosomes for a set of siblings in a large sibship. We've colored
  the parents' chromosomes as purple, blue, green, and orange. The
  chromosome from the mom is purple and blue; the chromosome from the
  data is green and orange. Two siblings share an allele IBD at a
  particular position if they got the same color. You can see that
  those large patches of color should mean that surrounding markers
  can be used to help determine IBD status.
}

\end{frame}





\begin{frame}{Average relative efficiency}
\figh{Figs/ave_rel_eff.png}{0.7}

\vspace{10mm}

\hfill \footnotesize {\lolit Broman (2001)
  \href{https://doi.org/10.1002/gepi.2}{doi:10.1002/gepi.2}}


\note{
  We can't assess the relative merits of methods 3 and 4 analytically,
  but we can carry out a simulation to see how well they work.
  This table shows the relative efficiency of each method, relative to
  the first method where we just use one sibling per pair. Methods 3
  and 4 improve a bit on method 2, and actually approach the best
  possible, a relative efficiency of 1.5. Methods 3 and 4 work
  somewhat better for less-frequent alleles, as it is easier to
  establish IBD in that case.
}

\end{frame}

\begin{frame}{Method 3}
\figh{Figs/rel_eff_method3.png}{0.6}

\vspace{18mm}

\hfill \footnotesize {\lolit Broman (2001)
  \href{https://doi.org/10.1002/gepi.2}{doi:10.1002/gepi.2}}


\note{
   Looking more closely at method 3, we note that its performance also
   depends on the amount of information at the marker, measured by the
   heterozygosity (the chance that an individual is heterozygous), as
   when heterozygosity is large, it's easier to infer IBD status.
}

\end{frame}

\begin{frame}{Method 4}
\figh{Figs/rel_eff_method4.png}{0.75}

\bigskip

\hfill \footnotesize {\lolit Broman (2001)
  \href{https://doi.org/10.1002/gepi.2}{doi:10.1002/gepi.2}}


\note{
  Looking more closely at method 4, we note that its performance
  improves when the marker density increases. ($d$ is the distance
  between markers, in cM.) With very dense markers, it achieves the
  ideal relative efficiency of 1.5.
}

\end{frame}


\begin{frame}[c]{Summary}
  \figw{Figs/summary_table.png}{0.7}

\note{
  Here's a different sort of summary of the results that includes the
  time to implement the methods in software, the time to actually
  obtain the estimates, and the average relative efficiency.

  Methods 3 and 4 are considerably more work to program. That you can
  implement method 3 in the morning and method 4 in the afternoon is
  perhaps overly optimistic; especially the latter. But it's maybe on
  that order. And while method 3 is slower than methods 1 and 2, it's
  still pretty close to instantaneous. Method 4, on the other hand, is
  like 1000 times slower.

  This is a commonly observed trade-off in data science: more complex
  methods can give improved estimates, but they take much longer to
  implement as well as execute. In this particular context, method 2
  is probably sufficient, but method 3 gives sufficiently improved
  estimates that it's probably worth it. Method 4 is clearly not worth
  it and was really just considered here because it's sort of cute.

  (It's good to remind you again here that when you think of
  programming time, it is good to think about the different
  contributions to that: formulating the problem, actually writing the
  software, then debugging the software, and finally running it.)
}

\end{frame}


\begin{frame}{One last thing}

  \bbi
\item Turns out, I made a {\hilit mistake} in Method 3
  \bi
\item[]  Mary Sara McPeek (U Chicago) spotted it
  \ei
\item Fixed problem, re-ran simulations, and... \\[18pt]
  \onslide<2->{the correct MLE was {\hilit worse} than my mistaken estimate}
 \ei


\note{
   A funny thing: it turns out that I messed up my formulation of
   method 4, and Mary Sara McPeek spotted it and pointed it out. I
   re-ran the simulations with the correction, and the true MLE actually
   performed worse than my mistaken version.
}

\end{frame}

\end{document}
